{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b74467e-eccf-4676-84e6-2a82da047d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6f2b0bd-eda5-4307-9b5b-ca28f9b64f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching logs...\n",
      "Fetched 667 logs\n",
      "\n",
      "Detecting anomalies...\n",
      "\n",
      "[DEBUG] Available columns in logs:\n",
      "['@timestamp', 'agent', 'log', 'input', 'message', 'sensor', 'timestamp', 'src_ip', 'session', 'eventid', 'ecs', 'host', 'duration', 'ttylog', 'duplicate', 'shasum', 'size', 'src_port', 'protocol', 'dst_port', 'dst_ip', 'version', 'kexAlgs', 'encCS', 'compCS', 'keyAlgs', 'hasshAlgorithms', 'macCS', 'langCS', 'hassh', 'username', 'password', 'height', 'width', 'arch']\n",
      "\n",
      "[DEBUG] Top 10 commands found:\n",
      "command\n",
      "Remote SSH version: SSH-2.0-OpenSSH_for_Windows_9.5               121\n",
      "SSH client hassh fingerprint: 701158e75b508e76f0410d5d22ef9df0    121\n",
      "Connection lost after 0.1 seconds                                  55\n",
      "cowrie.session.params                                              24\n",
      "Connection lost after 0.2 seconds                                  22\n",
      "login attempt [root/anything] succeeded                            16\n",
      "CMD: exit                                                          14\n",
      "CMD: whoami                                                        12\n",
      "CMD: pwd                                                           12\n",
      "CMD: cat /etc/passwd                                               12\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Analyzing 667 events with contamination=0.01\n",
      "\n",
      "=== SECURITY ALERTS ===\n",
      "Detected 10 suspicious events\n",
      "\n",
      "[ATTACK PATTERNS]\n",
      "Top malicious commands:\n",
      "command\n",
      "Remote SSH version: SSH-2.0-OpenSSH_for_Windows_9.5               5\n",
      "SSH client hassh fingerprint: 701158e75b508e76f0410d5d22ef9df0    5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top attacking IPs:\n",
      "src_ip\n",
      "172.19.0.1    6\n",
      "172.18.0.1    4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[SAMPLE EVENTS]\n",
      "                           timestamp      src_ip  \\\n",
      "123 2025-05-23 13:39:29.738415+00:00  172.18.0.1   \n",
      "124 2025-05-23 13:39:29.777265+00:00  172.18.0.1   \n",
      "152 2025-05-23 13:39:29.777265+00:00  172.18.0.1   \n",
      "190 2025-05-23 13:39:29.738415+00:00  172.18.0.1   \n",
      "194 2025-05-25 16:02:08.141266+00:00  172.19.0.1   \n",
      "\n",
      "                                               command  session_duration  \\\n",
      "123  Remote SSH version: SSH-2.0-OpenSSH_for_Window...          0.194360   \n",
      "124  SSH client hassh fingerprint: 701158e75b508e76...          0.194360   \n",
      "152  SSH client hassh fingerprint: 701158e75b508e76...          0.194360   \n",
      "190  Remote SSH version: SSH-2.0-OpenSSH_for_Window...          0.194360   \n",
      "194  Remote SSH version: SSH-2.0-OpenSSH_for_Window...         33.818402   \n",
      "\n",
      "     failures_per_ip  \n",
      "123              0.0  \n",
      "124              0.0  \n",
      "152              0.0  \n",
      "190              0.0  \n",
      "194              0.0  \n",
      "\n",
      "Saved 10 anomalies to anomalies.csv\n",
      "{'message_id': '<202505251603.28333587554@smtp-relay.mailin.fr>',\n",
      " 'message_ids': None}\n",
      "✅ Brevo alert sent successfully.\n",
      "✅ Security alerts sent successfully\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from elasticsearch import Elasticsearch\n",
    "from ipaddress import ip_network, ip_address\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configuration\n",
    "SUSPICIOUS_SUBNETS = [\"10.0.0.0/8\", \"192.168.0.0/16\"]\n",
    "FAILURE_THRESHOLD = 5\n",
    "MIN_SESSION_DURATION = 5\n",
    "MAX_SESSION_DURATION = 1800\n",
    "\n",
    "def safe_get(obj, key, default=\"\"):\n",
    "    \"\"\"Safely get value from nested dicts/lists.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return obj.get(key, default)\n",
    "    elif isinstance(obj, list):\n",
    "        return obj[0].get(key, default) if len(obj) > 0 else default\n",
    "    return default\n",
    "\n",
    "def fetch_logs(es_client, index_pattern=\"cowrie-*\", size=5000):\n",
    "    \"\"\"Fetch logs with error handling.\"\"\"\n",
    "    try:\n",
    "        response = es_client.search(\n",
    "            index=index_pattern,\n",
    "            body={\"query\": {\"match_all\": {}}, \"size\": size}\n",
    "        )\n",
    "        print(f\"Fetched {len(response['hits']['hits'])} logs\")\n",
    "        return response['hits']['hits']\n",
    "    except Exception as e:\n",
    "        print(f\"Elasticsearch error: {e}\")\n",
    "        return []\n",
    "\n",
    "def is_suspicious_ip(ip):\n",
    "    \"\"\"Check if IP is in suspicious ranges.\"\"\"\n",
    "    try:\n",
    "        return any(ip_address(ip) in ip_network(subnet) for subnet in SUSPICIOUS_SUBNETS)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def calculate_features(logs):\n",
    "    \"\"\"Engineer features with debug output.\"\"\"\n",
    "    df = pd.DataFrame([hit['_source'] for hit in logs])\n",
    "    \n",
    "    # Debug: Show available columns\n",
    "    print(\"\\n[DEBUG] Available columns in logs:\")\n",
    "    print(df.columns.tolist())\n",
    "    \n",
    "    # Timestamp features\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "    \n",
    "    # Enhanced command extraction\n",
    "    def get_command(x):\n",
    "        if isinstance(x.get('input'), str):\n",
    "            return x['input']\n",
    "        elif isinstance(x.get('input'), list) and len(x['input']) > 0:\n",
    "            return x['input'][0]\n",
    "        elif isinstance(x.get('message'), str):\n",
    "            return x['message']\n",
    "        return str(x.get('eventid', ''))\n",
    "\n",
    "    df['command'] = df.apply(get_command, axis=1)\n",
    "    print(\"\\n[DEBUG] Top 10 commands found:\")\n",
    "    print(df['command'].value_counts().head(10))\n",
    "    \n",
    "    # Command frequency\n",
    "    df['command_count'] = df['command'].map(\n",
    "        lambda x: df['command'].value_counts().get(x, 0))\n",
    "    \n",
    "    # IP analysis\n",
    "    df['src_ip'] = df['src_ip'].fillna('0.0.0.0')\n",
    "    df['is_private_ip'] = df['src_ip'].apply(is_suspicious_ip)\n",
    "    df['ip_frequency'] = df['src_ip'].map(\n",
    "        lambda x: df['src_ip'].value_counts().get(x, 0))\n",
    "    \n",
    "    # Session analysis\n",
    "    df['session'] = df['session'].fillna('no-session')\n",
    "    session_groups = df.groupby('session')['timestamp']\n",
    "    df['session_duration'] = session_groups.transform(\n",
    "        lambda x: (x.max() - x.min()).total_seconds()\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Failed logins\n",
    "    df['is_failed'] = df['eventid'] == 'cowrie.login.failed'\n",
    "    df['failures_per_ip'] = df['src_ip'].map(\n",
    "        df[df['is_failed']]['src_ip'].value_counts()).fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def detect_anomalies(logs):\n",
    "    \"\"\"Run anomaly detection with detailed reporting.\"\"\"\n",
    "    if not logs:\n",
    "        print(\"No logs to analyze\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        df = calculate_features(logs)\n",
    "        \n",
    "        # Feature selection\n",
    "        features = df[[\n",
    "            'hour',\n",
    "            'day_of_week',\n",
    "            'command_count',\n",
    "            'ip_frequency',\n",
    "            'failures_per_ip',\n",
    "            'session_duration'\n",
    "        ]].fillna(0).astype(float)\n",
    "        \n",
    "        # Dynamic contamination\n",
    "        contamination = min(0.1, max(0.01, 10 / len(df)))\n",
    "        print(f\"\\nAnalyzing {len(df)} events with contamination={contamination:.2f}\")\n",
    "        \n",
    "        model = IsolationForest(\n",
    "            contamination=contamination,\n",
    "            random_state=42,\n",
    "            n_estimators=200\n",
    "        )\n",
    "        df['anomaly_score'] = model.fit_predict(features)\n",
    "        df['is_anomaly'] = df['anomaly_score'] == -1\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Detection failed: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    es = Elasticsearch(\"http://localhost:9200\")\n",
    "    print(\"Fetching logs...\")\n",
    "    logs = fetch_logs(es)\n",
    "    \n",
    "    if logs:\n",
    "        print(\"\\nDetecting anomalies...\")\n",
    "        results = detect_anomalies(logs)\n",
    "        \n",
    "        if results is not None:\n",
    "            anomalies = results[results['is_anomaly']]\n",
    "            if not anomalies.empty:\n",
    "                print(\"\\n=== SECURITY ALERTS ===\")\n",
    "                print(f\"Detected {len(anomalies)} suspicious events\")\n",
    "                \n",
    "                print(\"\\n[ATTACK PATTERNS]\")\n",
    "                print(\"Top malicious commands:\")\n",
    "                print(anomalies['command'].value_counts().head(10))\n",
    "                \n",
    "                print(\"\\nTop attacking IPs:\")\n",
    "                print(anomalies['src_ip'].value_counts().head(5))\n",
    "                \n",
    "                print(\"\\n[SAMPLE EVENTS]\")\n",
    "                print(anomalies[['timestamp', 'src_ip', 'command', \n",
    "                               'session_duration', 'failures_per_ip']].head())\n",
    "                \n",
    "                # Save anomalies before alerting\n",
    "                anomalies_file = \"anomalies.csv\"\n",
    "                anomalies.to_csv(anomalies_file, index=False)\n",
    "                print(f\"\\nSaved {len(anomalies)} anomalies to {anomalies_file}\")\n",
    "                \n",
    "                # Alerting integration - FIXED VERSION\n",
    "                try:\n",
    "                    import os\n",
    "                    import sys\n",
    "                    from pathlib import Path\n",
    "                    \n",
    "                    # Get the current directory (works in both scripts and notebooks)\n",
    "                    try:\n",
    "                        script_dir = Path(__file__).absolute().parent\n",
    "                    except NameError:\n",
    "                        # Fallback for environments without __file__\n",
    "                        script_dir = Path(os.getcwd())\n",
    "                    \n",
    "                    # Navigate to project root and find alerting directory\n",
    "                    project_root = script_dir.parent\n",
    "                    alerting_path = str(project_root / \"alerting\")\n",
    "                    \n",
    "                    if alerting_path not in sys.path:\n",
    "                        sys.path.insert(0, alerting_path)\n",
    "                        \n",
    "                    from alert_manager import AlertManager\n",
    "                    \n",
    "                    alert_manager = AlertManager()\n",
    "                    if alert_manager.send_alert(anomalies_file):\n",
    "                        print(\"✅ Security alerts sent successfully\")\n",
    "                    else:\n",
    "                        print(\"⚠️ Alert sending failed\")\n",
    "                except ImportError as e:\n",
    "                    print(f\"⚠️ Alerting module import failed: {e}\")\n",
    "                    print(\"Please ensure:\")\n",
    "                    print(f\"1. The 'alerting' directory exists at: {alerting_path}\")\n",
    "                    print(\"2. It contains 'alert_manager.py' with proper email configuration\")\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Alerting failed: {str(e)}\")\n",
    "            else:\n",
    "                print(\"No anomalies detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6669865e-318b-404f-b85a-d092cbb70457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d1f94f7-2bba-4609-a87e-8a377d7a92b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching logs...\n",
      "Fetched 679 logs\n",
      "\n",
      "Detecting anomalies...\n",
      "\n",
      "Analyzing 679 events with contamination=0.01\n",
      "\n",
      "=== SECURITY ALERTS ===\n",
      "Detected 8 suspicious events\n",
      "\n",
      "[ATTACK PATTERNS]\n",
      "Top malicious commands:\n",
      "command\n",
      "Remote SSH version: SSH-2.0-OpenSSH_for_Windows_9.5               4\n",
      "SSH client hassh fingerprint: 701158e75b508e76f0410d5d22ef9df0    4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top attacking IPs:\n",
      "src_ip\n",
      "172.18.0.1    4\n",
      "172.19.0.1    4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[SAMPLE EVENTS]\n",
      "                           timestamp      src_ip  \\\n",
      "123 2025-05-23 13:39:29.738415+00:00  172.18.0.1   \n",
      "124 2025-05-23 13:39:29.777265+00:00  172.18.0.1   \n",
      "152 2025-05-23 13:39:29.777265+00:00  172.18.0.1   \n",
      "190 2025-05-23 13:39:29.738415+00:00  172.18.0.1   \n",
      "199 2025-05-25 16:11:53.715836+00:00  172.19.0.1   \n",
      "\n",
      "                                               command  session_duration  \\\n",
      "123  Remote SSH version: SSH-2.0-OpenSSH_for_Window...          0.194360   \n",
      "124  SSH client hassh fingerprint: 701158e75b508e76...          0.194360   \n",
      "152  SSH client hassh fingerprint: 701158e75b508e76...          0.194360   \n",
      "190  Remote SSH version: SSH-2.0-OpenSSH_for_Window...          0.194360   \n",
      "199  Remote SSH version: SSH-2.0-OpenSSH_for_Window...         54.829859   \n",
      "\n",
      "     failures_per_ip  \n",
      "123              0.0  \n",
      "124              0.0  \n",
      "152              0.0  \n",
      "190              0.0  \n",
      "199              0.0  \n",
      "\n",
      "Saved 8 anomalies to anomalies.csv\n",
      "ℹ️ Streamlit dashboard is already running.\n",
      "{'message_id': '<202505251613.41766700272@smtp-relay.mailin.fr>',\n",
      " 'message_ids': None}\n",
      "✅ Brevo alert sent successfully.\n",
      "✅ Security alerts sent successfully\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from elasticsearch import Elasticsearch\n",
    "from ipaddress import ip_network, ip_address\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configuration\n",
    "SUSPICIOUS_SUBNETS = [\"10.0.0.0/8\", \"192.168.0.0/16\"]\n",
    "FAILURE_THRESHOLD = 5\n",
    "MIN_SESSION_DURATION = 5\n",
    "MAX_SESSION_DURATION = 1800\n",
    "\n",
    "def safe_get(obj, key, default=\"\"):\n",
    "    \"\"\"Safely get value from nested dicts/lists.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return obj.get(key, default)\n",
    "    elif isinstance(obj, list) and len(obj) > 0 and isinstance(obj[0], dict):\n",
    "        return obj[0].get(key, default)\n",
    "    return default\n",
    "\n",
    "def fetch_logs(es_client, index_pattern=\"cowrie-*\", size=5000):\n",
    "    \"\"\"Fetch logs with error handling.\"\"\"\n",
    "    try:\n",
    "        response = es_client.search(\n",
    "            index=index_pattern,\n",
    "            body={\"query\": {\"match_all\": {}}, \"size\": size}\n",
    "        )\n",
    "        hits = response.get('hits', {}).get('hits', [])\n",
    "        print(f\"Fetched {len(hits)} logs\")\n",
    "        return hits\n",
    "    except Exception as e:\n",
    "        print(f\"Elasticsearch error: {e}\")\n",
    "        return []\n",
    "\n",
    "def is_suspicious_ip(ip):\n",
    "    \"\"\"Check if IP is in suspicious ranges.\"\"\"\n",
    "    try:\n",
    "        return any(ip_address(ip) in ip_network(subnet) for subnet in SUSPICIOUS_SUBNETS)\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def calculate_features(logs):\n",
    "    \"\"\"Engineer features with debug output.\"\"\"\n",
    "    df = pd.DataFrame([hit.get('_source', {}) for hit in logs])\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No log data available to process.\")\n",
    "    \n",
    "    # Timestamp features\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "    df = df.dropna(subset=['timestamp'])  # Drop rows where timestamp couldn't be parsed\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "    \n",
    "    # Enhanced command extraction\n",
    "    def get_command(row):\n",
    "        input_val = row.get('input')\n",
    "        if isinstance(input_val, str):\n",
    "            return input_val\n",
    "        elif isinstance(input_val, list) and len(input_val) > 0:\n",
    "            return input_val[0]\n",
    "        elif isinstance(row.get('message'), str):\n",
    "            return row['message']\n",
    "        return str(row.get('eventid', ''))\n",
    "\n",
    "    df['command'] = df.apply(get_command, axis=1)\n",
    "    \n",
    "    # Command frequency\n",
    "    command_counts = df['command'].value_counts()\n",
    "    df['command_count'] = df['command'].map(command_counts)\n",
    "    \n",
    "    # IP analysis\n",
    "    df['src_ip'] = df['src_ip'].fillna('0.0.0.0')\n",
    "    df['is_private_ip'] = df['src_ip'].apply(is_suspicious_ip)\n",
    "    ip_counts = df['src_ip'].value_counts()\n",
    "    df['ip_frequency'] = df['src_ip'].map(ip_counts)\n",
    "    \n",
    "    # Session analysis\n",
    "    df['session'] = df['session'].fillna('no-session')\n",
    "    session_groups = df.groupby('session')['timestamp']\n",
    "    df['session_duration'] = session_groups.transform(\n",
    "        lambda x: (x.max() - x.min()).total_seconds()\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Failed logins\n",
    "    df['is_failed'] = df['eventid'] == 'cowrie.login.failed'\n",
    "    failed_counts = df[df['is_failed']]['src_ip'].value_counts()\n",
    "    df['failures_per_ip'] = df['src_ip'].map(failed_counts).fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def detect_anomalies(logs):\n",
    "    \"\"\"Run anomaly detection with detailed reporting.\"\"\"\n",
    "    if not logs:\n",
    "        print(\"No logs to analyze\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        df = calculate_features(logs)\n",
    "        \n",
    "        # Feature selection\n",
    "        features = df[[\n",
    "            'hour',\n",
    "            'day_of_week',\n",
    "            'command_count',\n",
    "            'ip_frequency',\n",
    "            'failures_per_ip',\n",
    "            'session_duration'\n",
    "        ]].fillna(0).astype(float)\n",
    "        \n",
    "        # Dynamic contamination rate\n",
    "        contamination = min(0.1, max(0.01, 10 / len(df)))\n",
    "        print(f\"\\nAnalyzing {len(df)} events with contamination={contamination:.2f}\")\n",
    "        \n",
    "        model = IsolationForest(\n",
    "            contamination=contamination,\n",
    "            random_state=42,\n",
    "            n_estimators=200\n",
    "        )\n",
    "        df['anomaly_score'] = model.fit_predict(features)\n",
    "        df['is_anomaly'] = df['anomaly_score'] == -1\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Detection failed: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    es = Elasticsearch(\"http://localhost:9200\")\n",
    "    print(\"Fetching logs...\")\n",
    "    logs = fetch_logs(es)\n",
    "    \n",
    "    if logs:\n",
    "        print(\"\\nDetecting anomalies...\")\n",
    "        results = detect_anomalies(logs)\n",
    "        \n",
    "        if results is not None:\n",
    "            anomalies = results[results['is_anomaly']]\n",
    "            if not anomalies.empty:\n",
    "                print(\"\\n=== SECURITY ALERTS ===\")\n",
    "                print(f\"Detected {len(anomalies)} suspicious events\")\n",
    "                \n",
    "                print(\"\\n[ATTACK PATTERNS]\")\n",
    "                print(\"Top malicious commands:\")\n",
    "                print(anomalies['command'].value_counts().head(10))\n",
    "                \n",
    "                print(\"\\nTop attacking IPs:\")\n",
    "                print(anomalies['src_ip'].value_counts().head(5))\n",
    "                \n",
    "                print(\"\\n[SAMPLE EVENTS]\")\n",
    "                print(anomalies[['timestamp', 'src_ip', 'command', \n",
    "                               'session_duration', 'failures_per_ip']].head())\n",
    "                \n",
    "                # Save anomalies before alerting\n",
    "                anomalies_file = \"anomalies.csv\"\n",
    "                anomalies.to_csv(anomalies_file, index=False)\n",
    "                print(f\"\\nSaved {len(anomalies)} anomalies to {anomalies_file}\")\n",
    "\n",
    "                import subprocess\n",
    "                import psutil\n",
    "                \n",
    "                def is_streamlit_running(script_name=\"app.py\"):\n",
    "                    \"\"\"Check if Streamlit is already running with the given script.\"\"\"\n",
    "                    for proc in psutil.process_iter(attrs=[\"cmdline\"]):\n",
    "                        try:\n",
    "                            cmdline = proc.info[\"cmdline\"]\n",
    "                            if cmdline and \"streamlit\" in cmdline[0] and script_name in \" \".join(cmdline):\n",
    "                                return True\n",
    "                        except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
    "                            continue\n",
    "                    return False\n",
    "                \n",
    "                # ✅ Launch Streamlit only if it's not already running\n",
    "                try:\n",
    "                    if not is_streamlit_running(\"app.py\"):\n",
    "                        print(\"\\n🚀 Launching Streamlit dashboard...\")\n",
    "                        subprocess.Popen([\"streamlit\", \"run\", \"app.py\"])\n",
    "                        print(\"✅ Streamlit dashboard launched.\")\n",
    "                    else:\n",
    "                        print(\"ℹ️ Streamlit dashboard is already running.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Failed to launch dashboard: {e}\")\n",
    "\n",
    "                # Alerting integration\n",
    "                try:\n",
    "                    import os\n",
    "                    import sys\n",
    "                    from pathlib import Path\n",
    "                    \n",
    "                    try:\n",
    "                        script_dir = Path(__file__).absolute().parent\n",
    "                    except NameError:\n",
    "                        script_dir = Path(os.getcwd())\n",
    "                    \n",
    "                    project_root = script_dir.parent\n",
    "                    alerting_path = str(project_root / \"alerting\")\n",
    "                    \n",
    "                    if alerting_path not in sys.path:\n",
    "                        sys.path.insert(0, alerting_path)\n",
    "                        \n",
    "                    from alert_manager import AlertManager\n",
    "                    \n",
    "                    alert_manager = AlertManager()\n",
    "                    if alert_manager.send_alert(anomalies_file):\n",
    "                        print(\"✅ Security alerts sent successfully\")\n",
    "                    else:\n",
    "                        print(\"⚠️ Alert sending failed\")\n",
    "                except ImportError as e:\n",
    "                    print(f\"⚠️ Alerting module import failed: {e}\")\n",
    "                    print(\"Please ensure:\")\n",
    "                    print(f\"1. The 'alerting' directory exists at: {alerting_path}\")\n",
    "                    print(\"2. It contains 'alert_manager.py' with proper email configuration\")\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Alerting failed: {str(e)}\")\n",
    "            else:\n",
    "                print(\"No anomalies detected\")\n",
    "        else:\n",
    "            print(\"Error during anomaly detection\")\n",
    "    else:\n",
    "        print(\"No logs fetched from Elasticsearch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe38ac2-7d72-4e1a-a221-fca5c0f9f6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "909445c9-489d-4932-8cf8-c052f2e34863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (5.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8f103a-b1ae-481e-b6dc-42116f1d9a01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
